#!/usr/bin/env python3
"""
æŸ¥è¯¢å¸‚åœºæ–°é—»å’Œæƒ…ç»ª - é€šè¿‡ AlphaVantage NEWS_SENTIMENT API

ç”¨æ³•:
    python query_market_news.py                          # æŸ¥è¯¢æœ€æ–°é‡‘èå¸‚åœºæ–°é—»
    python query_market_news.py --tickers AAPL,NVDA      # æŸ¥è¯¢æŒ‡å®šè‚¡ç¥¨ç›¸å…³æ–°é—»
    python query_market_news.py --topics technology       # æŸ¥è¯¢æŒ‡å®šä¸»é¢˜æ–°é—»
    python query_market_news.py --tickers AAPL --topics earnings  # ç»„åˆè¿‡æ»¤
"""

import sys
import json
import argparse
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

# å°† scripts ç›®å½•åŠ å…¥ Python è·¯å¾„ä»¥å¯¼å…¥ _config
sys.path.insert(0, str(Path(__file__).resolve().parent))

from _config import load_config, get_alphavantage_key

# ä» config.yaml åŠ è½½ AlphaVantage API Key
_config = load_config()
APIKEY = get_alphavantage_key(_config)
BASE_URL = "https://www.alphavantage.co/query"
DEFAULT_ALPHA_REQUEST_INTERVAL = 0.8  # ä»˜è´¹ç‰ˆ 75 æ¬¡/åˆ†é’Ÿ

# æ”¯æŒçš„æ–°é—»ä¸»é¢˜
SUPPORTED_TOPICS = [
    "blockchain", "earnings", "ipo", "mergers_and_acquisitions",
    "financial_markets", "economy_fiscal", "economy_monetary", "economy_macro",
    "energy_transportation", "finance", "life_sciences", "manufacturing",
    "real_estate", "retail_wholesale", "technology"
]


def _split_tickers(tickers: Optional[str]) -> List[str]:
    if not tickers:
        return []
    return [t.strip().upper() for t in tickers.split(",") if t.strip()]


def _extract_ticker_sentiment(article: Dict[str, Any], ticker: str) -> Optional[float]:
    ticker = ticker.upper()
    for item in article.get("ticker_sentiment", []):
        if str(item.get("ticker", "")).upper() == ticker:
            try:
                return float(item.get("ticker_sentiment_score"))
            except (TypeError, ValueError):
                return None
    return None


def fetch_news_per_ticker(
    tickers: List[str],
    per_ticker_limit: int = 5,
    topics: Optional[str] = None,
    time_from: Optional[str] = None,
    time_to: Optional[str] = None,
    sort: str = "LATEST",
    request_interval: float = 0.0,
) -> List[Dict[str, Any]]:
    """
    é€åªè‚¡ç¥¨æŸ¥è¯¢æ–°é—»ï¼Œè¿”å›æŒ‰ ticker åˆ†ç»„ç»“æœï¼ˆåˆ†æå‰ç½®åœºæ™¯ï¼‰ã€‚
    """
    grouped_results: List[Dict[str, Any]] = []
    for idx, ticker in enumerate(tickers):
        articles = fetch_news(
            tickers=ticker,
            topics=topics,
            time_from=time_from,
            time_to=time_to,
            sort=sort,
            limit=per_ticker_limit,
        )

        normalized_articles: List[Dict[str, Any]] = []
        overall_scores: List[float] = []
        ticker_scores: List[float] = []

        for a in articles:
            try:
                overall_score = float(a.get("overall_sentiment_score", 0))
                overall_scores.append(overall_score)
            except (TypeError, ValueError):
                overall_score = None

            ticker_score = _extract_ticker_sentiment(a, ticker)
            if ticker_score is not None:
                ticker_scores.append(ticker_score)

            normalized_articles.append(
                {
                    "title": a.get("title", ""),
                    "url": a.get("url", ""),
                    "source": a.get("source", ""),
                    "time_published": a.get("time_published", ""),
                    "time_published_readable": parse_time_published(a.get("time_published", "")),
                    "overall_sentiment_label": a.get("overall_sentiment_label", ""),
                    "overall_sentiment_score": overall_score,
                    "target_ticker_sentiment_score": ticker_score,
                    "target_ticker_sentiment_label": (
                        next(
                            (
                                s.get("ticker_sentiment_label", "")
                                for s in a.get("ticker_sentiment", [])
                                if str(s.get("ticker", "")).upper() == ticker
                            ),
                            "",
                        )
                    ),
                    "summary": a.get("summary", ""),
                }
            )

        grouped_results.append(
            {
                "ticker": ticker,
                "news_count": len(normalized_articles),
                "avg_overall_sentiment_score": (
                    round(sum(overall_scores) / len(overall_scores), 6) if overall_scores else None
                ),
                "avg_ticker_sentiment_score": (
                    round(sum(ticker_scores) / len(ticker_scores), 6) if ticker_scores else None
                ),
                "articles": normalized_articles,
            }
        )

        if request_interval > 0 and idx < len(tickers) - 1:
            time.sleep(request_interval)

    return grouped_results


def fetch_news(
    tickers: Optional[str] = None,
    topics: Optional[str] = None,
    time_from: Optional[str] = None,
    time_to: Optional[str] = None,
    sort: str = "LATEST",
    limit: int = 20,
) -> List[Dict[str, Any]]:
    """
    ä» AlphaVantage NEWS_SENTIMENT API è·å–æ–°é—»

    Args:
        tickers: è‚¡ç¥¨ä»£ç ï¼Œé€—å·åˆ†éš” (ä¾‹: "AAPL" æˆ– "AAPL,NVDA,CRYPTO:BTC")
        topics: æ–°é—»ä¸»é¢˜ï¼Œé€—å·åˆ†éš” (ä¾‹: "technology" æˆ– "technology,earnings")
        time_from: èµ·å§‹æ—¶é—´ï¼Œæ ¼å¼ YYYYMMDDTHHMM (ä¾‹: "20260101T0000")
        time_to: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ YYYYMMDDTHHMM
        sort: æ’åºæ–¹å¼ ("LATEST", "EARLIEST", "RELEVANCE")
        limit: è¿”å›æ•°é‡ä¸Šé™

    Returns:
        æ–°é—»æ–‡ç« åˆ—è¡¨
    """
    params = {
        "function": "NEWS_SENTIMENT",
        "apikey": APIKEY,
        "sort": sort,
        "limit": limit,
    }

    if tickers:
        params["tickers"] = tickers
    if topics:
        params["topics"] = topics
    if time_from:
        params["time_from"] = time_from
    if time_to:
        params["time_to"] = time_to

    try:
        response = requests.get(BASE_URL, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if "Error Message" in data:
            raise Exception(f"API é”™è¯¯: {data['Error Message']}")
        if "Note" in data:
            raise Exception(f"API è°ƒç”¨é™åˆ¶: {data['Note']}")

        feed = data.get("feed", [])
        return feed[:limit]

    except requests.exceptions.Timeout:
        raise Exception("è¯·æ±‚è¶…æ—¶")
    except requests.exceptions.RequestException as e:
        raise Exception(f"è¯·æ±‚å¤±è´¥: {e}")


def parse_time_published(time_str: str) -> str:
    """
    è§£æ AlphaVantage æ—¶é—´æ ¼å¼ä¸ºå¯è¯»æ ¼å¼

    Args:
        time_str: AlphaVantage æ—¶é—´å­—ç¬¦ä¸² (ä¾‹: "20260205T143000")

    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
    """
    try:
        if "T" in time_str:
            date_part = time_str.split("T")[0]
            time_part = time_str.split("T")[1]
            if len(date_part) == 8:
                if len(time_part) >= 6:
                    dt = datetime.strptime(time_str[:15], "%Y%m%dT%H%M%S")
                elif len(time_part) >= 4:
                    dt = datetime.strptime(time_str[:13], "%Y%m%dT%H%M")
                else:
                    return time_str
                return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        pass
    return time_str


def format_sentiment(score: float) -> str:
    """
    å°†æƒ…ç»ªè¯„åˆ†æ ¼å¼åŒ–ä¸ºæè¿°æ€§æ–‡æœ¬

    Args:
        score: æƒ…ç»ªè¯„åˆ† (-1 åˆ° 1)

    Returns:
        æƒ…ç»ªæè¿°
    """
    if score >= 0.35:
        return f"å¼ºçƒˆçœ‹æ¶¨ ({score:+.3f})"
    elif score >= 0.15:
        return f"çœ‹æ¶¨ ({score:+.3f})"
    elif score >= -0.15:
        return f"ä¸­æ€§ ({score:+.3f})"
    elif score >= -0.35:
        return f"çœ‹è·Œ ({score:+.3f})"
    else:
        return f"å¼ºçƒˆçœ‹è·Œ ({score:+.3f})"


def display_articles(articles: List[Dict[str, Any]], verbose: bool = False):
    """
    æ ¼å¼åŒ–æ˜¾ç¤ºæ–°é—»æ–‡ç« 

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    if not articles:
        print("  (æ— åŒ¹é…çš„æ–°é—»)")
        return

    for i, article in enumerate(articles, 1):
        title = article.get("title", "N/A")
        source = article.get("source", "N/A")
        time_published = parse_time_published(article.get("time_published", ""))
        summary = article.get("summary", "")
        overall_sentiment = article.get("overall_sentiment_score", 0)
        sentiment_label = article.get("overall_sentiment_label", "N/A")

        print(f"\n  {i}. {title}")
        print(f"     æ¥æº: {source} | æ—¶é—´: {time_published}")

        # æƒ…ç»ªè¯„åˆ†
        try:
            score = float(overall_sentiment)
            print(f"     æƒ…ç»ª: {format_sentiment(score)}")
        except (ValueError, TypeError):
            print(f"     æƒ…ç»ª: {sentiment_label}")

        # æ‘˜è¦ï¼ˆæˆªæ–­åˆ° 200 å­—ç¬¦ï¼‰
        if summary:
            display_summary = summary[:200] + "..." if len(summary) > 200 else summary
            print(f"     æ‘˜è¦: {display_summary}")

        # è¯¦ç»†æ¨¡å¼ï¼šæ˜¾ç¤ºä¸ªè‚¡æƒ…ç»ª
        if verbose:
            ticker_sentiment = article.get("ticker_sentiment", [])
            if ticker_sentiment:
                print("     ä¸ªè‚¡æƒ…ç»ª:")
                for ts in ticker_sentiment[:5]:
                    ticker = ts.get("ticker", "N/A")
                    relevance = ts.get("relevance_score", "N/A")
                    t_score = ts.get("ticker_sentiment_score", "N/A")
                    t_label = ts.get("ticker_sentiment_label", "N/A")
                    print(f"       {ticker}: {t_label} (score={t_score}, relevance={relevance})")

            topics_list = article.get("topics", [])
            if topics_list:
                topics_str = ", ".join([t.get("topic", "") for t in topics_list])
                print(f"     ä¸»é¢˜: {topics_str}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æŸ¥è¯¢å¸‚åœºæ–°é—»å’Œæƒ…ç»ª (AlphaVantage NEWS_SENTIMENT API)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
æ”¯æŒçš„ä¸»é¢˜ (--topics):
  {', '.join(SUPPORTED_TOPICS)}

ç¤ºä¾‹:
  python query_market_news.py --tickers AAPL
  python query_market_news.py --tickers NVDA,AMD --topics technology
  python query_market_news.py --topics earnings --limit 5
  python query_market_news.py --tickers CRYPTO:BTC --verbose
"""
    )
    parser.add_argument("--tickers", type=str, default=None,
                        help="è‚¡ç¥¨ä»£ç ï¼Œé€—å·åˆ†éš” (ä¾‹: AAPL,NVDA,CRYPTO:BTC)")
    parser.add_argument("--topics", type=str, default=None,
                        help="æ–°é—»ä¸»é¢˜ï¼Œé€—å·åˆ†éš” (ä¾‹: technology,earnings)")
    parser.add_argument("--days", type=int, default=7,
                        help="æŸ¥è¯¢æœ€è¿‘ N å¤©çš„æ–°é—» (é»˜è®¤: 7)")
    parser.add_argument("--limit", type=int, default=10,
                        help="è¿”å›æ•°é‡ (é»˜è®¤: 10, æœ€å¤§: 50)")
    parser.add_argument("--sort", type=str, default="LATEST",
                        choices=["LATEST", "EARLIEST", "RELEVANCE"],
                        help="æ’åºæ–¹å¼ (é»˜è®¤: LATEST)")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆä¸ªè‚¡æƒ…ç»ªã€ä¸»é¢˜ç­‰ï¼‰")
    parser.add_argument("--json", action="store_true",
                        help="ä»¥ JSON æ ¼å¼è¾“å‡º")
    parser.add_argument(
        "--per-ticker",
        action="store_true",
        help="æŒ‰è‚¡ç¥¨é€ä¸ªæŸ¥è¯¢å¹¶åˆ†ç»„è¾“å‡ºï¼ˆåˆ†æå‰æ¨èæ¨¡å¼ï¼‰",
    )
    parser.add_argument(
        "--per-ticker-limit",
        type=int,
        default=5,
        help="é€è‚¡ç¥¨æ¨¡å¼ä¸‹ï¼Œæ¯åªè‚¡ç¥¨è¿”å›æ¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰",
    )
    parser.add_argument(
        "--request-interval",
        type=float,
        default=DEFAULT_ALPHA_REQUEST_INTERVAL,
        help="é€è‚¡ç¥¨æ¨¡å¼ä¸‹ï¼Œæ¯æ¬¡ API è°ƒç”¨é—´éš”ç§’æ•°ï¼ˆé»˜è®¤: 0.8ï¼Œçº¦ 75æ¬¡/åˆ†é’Ÿï¼‰",
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="",
        help="å°†ç»“æœå†™å…¥æŒ‡å®š JSON æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
    )
    args = parser.parse_args()

    print("ğŸ“° å¸‚åœºæ–°é—»ä¸æƒ…ç»ªæŸ¥è¯¢")
    print("=" * 60)
    print(f"æ•°æ®æ¥æº: AlphaVantage NEWS_SENTIMENT API")
    print(f"æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    filters = []
    if args.tickers:
        filters.append(f"è‚¡ç¥¨: {args.tickers}")
    if args.topics:
        filters.append(f"ä¸»é¢˜: {args.topics}")
    if filters:
        print(f"è¿‡æ»¤æ¡ä»¶: {' | '.join(filters)}")
    print(f"æ’åº: {args.sort} | æ•°é‡: {args.limit}")
    print("=" * 60)

    # è®¡ç®—æ—¶é—´èŒƒå›´
    now = datetime.now()
    time_from = (now - timedelta(days=args.days)).strftime("%Y%m%dT0000")
    time_to = now.strftime("%Y%m%dT%H%M")

    try:
        print(f"\nè·å–æœ€è¿‘ {args.days} å¤©çš„æ–°é—»...\n")
        if args.per_ticker:
            parsed_tickers = _split_tickers(args.tickers)
            if not parsed_tickers:
                raise Exception("é€è‚¡ç¥¨æ¨¡å¼å¿…é¡»æä¾› --tickersï¼Œä¾‹å¦‚ --tickers NVDA,MSFT,AAPL")

            per_ticker_limit = max(1, min(args.per_ticker_limit, 50))
            grouped = fetch_news_per_ticker(
                tickers=parsed_tickers,
                per_ticker_limit=per_ticker_limit,
                topics=args.topics,
                time_from=time_from,
                time_to=time_to,
                sort=args.sort,
                request_interval=max(args.request_interval, 0.0),
            )

            payload = {
                "mode": "per_ticker",
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "days": args.days,
                "sort": args.sort,
                "per_ticker_limit": per_ticker_limit,
                "tickers": parsed_tickers,
                "results": grouped,
            }

            if args.output_file:
                out_path = Path(args.output_file)
                out_path.parent.mkdir(parents=True, exist_ok=True)
                out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8")
                print(f"ğŸ’¾ å·²å†™å…¥: {out_path}")

            if args.json:
                print(json.dumps(payload, indent=2, ensure_ascii=False))
            else:
                for item in grouped:
                    ticker = item["ticker"]
                    print(f"ğŸ“Œ {ticker} | æ–°é—»æ•°: {item['news_count']}")
                    if item["avg_overall_sentiment_score"] is not None:
                        print(f"  å¹³å‡æ–°é—»æƒ…ç»ª: {item['avg_overall_sentiment_score']:+.3f}")
                    if item["avg_ticker_sentiment_score"] is not None:
                        print(f"  å¹³å‡ä¸ªè‚¡æƒ…ç»ª: {item['avg_ticker_sentiment_score']:+.3f}")
                    display_articles(item["articles"], verbose=args.verbose)
                    print()
        else:
            articles = fetch_news(
                tickers=args.tickers,
                topics=args.topics,
                time_from=time_from,
                time_to=time_to,
                sort=args.sort,
                limit=min(args.limit, 50),
            )
            if args.output_file:
                out_path = Path(args.output_file)
                out_path.parent.mkdir(parents=True, exist_ok=True)
                out_path.write_text(json.dumps(articles, indent=2, ensure_ascii=False), encoding="utf-8")
                print(f"ğŸ’¾ å·²å†™å…¥: {out_path}")

            if args.json:
                print(json.dumps(articles, indent=2, ensure_ascii=False))
            else:
                print(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–°é—»:")
                display_articles(articles, verbose=args.verbose)

    except Exception as e:
        print(f"\nâŒ æŸ¥è¯¢å¤±è´¥: {e}")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ’¡ æç¤º: AlphaVantage å…è´¹ç‰ˆé™åˆ¶ 25 æ¬¡/å¤©ï¼Œå¦‚é‡é™åˆ¶è¯·ç¨åé‡è¯•")
    print("   æƒ…ç»ªè¯„åˆ†èŒƒå›´: -1 (å¼ºçƒˆçœ‹è·Œ) åˆ° +1 (å¼ºçƒˆçœ‹æ¶¨)")


if __name__ == "__main__":
    main()
